name: transformer
description: 2-layer Transformer encoder baseline

architecture:
  d_model: 64
  nhead: 4
  num_layers: 2
  dim_feedforward: 256
  dropout: 0.1

training:
  style: dl
  epochs: 100
  learning_rate: 0.0005
  optimizer: adam
  weight_decay: 0.0001
  early_stopping_patience: 10
